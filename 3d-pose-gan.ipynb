{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Initial code was copied from:\n",
    "https://github.com/jason71995/Keras-GAN-Library\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,GlobalAveragePooling2D,LeakyReLU,Conv2DTranspose,Activation,BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras import initializers\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "\n",
    "\n",
    "noise_dim = 5\n",
    "\n",
    "def build_generator(input_shape):\n",
    "\n",
    "    generator = Sequential()\n",
    "    \n",
    "    generator.add(Dense(64,\n",
    "                        input_dim = noise_dim))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "#     generator.add(Dense(512))\n",
    "#     generator.add(BatchNormalization())\n",
    "#     generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    generator.add(Dense(105, activation='tanh'))\n",
    "    return generator\n",
    "\n",
    "\n",
    "def build_discriminator(input_shape):\n",
    "\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Dense(64,\n",
    "                            input_dim=105))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "    \n",
    "    \n",
    "#     discriminator.add(Dense(512))\n",
    "#     discriminator.add(LeakyReLU(0.2))\n",
    "#     discriminator.add(Dropout(0.3))\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return discriminator\n",
    "\n",
    "def build_functions(batch_size, noise_size, image_size, generator, discriminator):\n",
    "\n",
    "    noise = K.random_normal((batch_size,) + noise_size,0.0,1.0,\"float32\")\n",
    "    real_image = K.placeholder((batch_size,) + image_size)\n",
    "\n",
    "    fake_image = generator(noise)\n",
    "\n",
    "    d_input = K.concatenate([real_image, fake_image], axis=0)\n",
    "    pred_real, pred_fake = tf.split(discriminator(d_input), num_or_size_splits = 2, axis = 0)\n",
    "\n",
    "    pred_real = K.clip(pred_real,K.epsilon(),1-K.epsilon())\n",
    "    pred_fake = K.clip(pred_fake,K.epsilon(),1-K.epsilon())\n",
    "\n",
    "    d_loss = -(K.mean(K.log(pred_real)) + K.mean(K.log(1-pred_fake)))\n",
    "    g_loss = -K.mean(K.log(pred_fake))\n",
    "\n",
    "    # get updates of mean and variance in batch normalization layers\n",
    "    d_updates = discriminator.get_updates_for([d_input])\n",
    "    g_updates = generator.get_updates_for([noise])\n",
    "\n",
    "    d_training_updates = Adam(lr=0.0001, beta_1=0.0, beta_2=0.9).get_updates(d_loss, discriminator.trainable_weights)\n",
    "    d_train = K.function([real_image, K.learning_phase()], [d_loss],d_updates + d_training_updates)\n",
    "\n",
    "    g_training_updates = Adam(lr=0.0001, beta_1=0.0, beta_2=0.9).get_updates(g_loss, generator.trainable_weights)\n",
    "    g_train = K.function([real_image, K.learning_phase()], [g_loss], g_updates + g_training_updates)\n",
    "\n",
    "    return d_train,g_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117562, 105)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading MHAD data for action1, all persons and all repeatations of each person\n",
    "from utils.data_loader import data_loader\n",
    "data_object= data_loader(matlab_action_path='../gan/')\n",
    "myData, mymin, mymax = data_object.actions_normalised([1], twoD_true_or_threeD_false=False)\n",
    "myData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Batches passed in each epoch:  7347\n",
      "[0/26] d_loss: 1.151, g_loss: 0.7963\n",
      "\n",
      "\n",
      "Time Taken for single epoch:\n",
      "00:00:54\n",
      "[1/26] d_loss: 1.295, g_loss: 0.7525\n",
      "[2/26] d_loss: 1.315, g_loss: 0.6759\n",
      "[3/26] d_loss: 1.268, g_loss: 0.7669\n",
      "[4/26] d_loss: 1.324, g_loss: 0.713\n",
      "[5/26] d_loss: 1.344, g_loss: 0.7542\n",
      "[6/26] d_loss: 1.337, g_loss: 0.7468\n",
      "[7/26] d_loss: 1.373, g_loss: 0.7338\n",
      "[8/26] d_loss: 1.306, g_loss: 0.703\n",
      "[9/26] d_loss: 1.304, g_loss: 0.8106\n",
      "[10/26] d_loss: 1.325, g_loss: 0.7296\n",
      "[11/26] d_loss: 1.303, g_loss: 0.7497\n",
      "[12/26] d_loss: 1.346, g_loss: 0.6764\n",
      "[13/26] d_loss: 1.262, g_loss: 0.7625\n",
      "[14/26] d_loss: 1.303, g_loss: 0.7475\n",
      "[15/26] d_loss: 1.212, g_loss: 0.7231\n",
      "[16/26] d_loss: 1.239, g_loss: 0.8326\n",
      "[17/26] d_loss: 1.334, g_loss: 1.014\n",
      "[18/26] d_loss: 1.315, g_loss: 0.8033\n",
      "[19/26] d_loss: 1.255, g_loss: 0.9312\n",
      "[20/26] d_loss: 1.28, g_loss: 0.9483\n",
      "[21/26] d_loss: 1.275, g_loss: 0.7471\n",
      "[22/26] d_loss: 1.271, g_loss: 0.8632\n",
      "[23/26] d_loss: 1.391, g_loss: 1.01\n",
      "[24/26] d_loss: 1.29, g_loss: 0.8082\n",
      "[25/26] d_loss: 1.24, g_loss: 0.754\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Number of parameter for the Generator and discriminator respectively:\n",
      "\n",
      "7465\n",
      "\n",
      "6849\n",
      "\n",
      "\n",
      "Number of Epochs and steps for each epoch:\n",
      "\n",
      "epochs:  26    batches:  7347\n",
      "\n",
      "\n",
      "Time Taken:\n",
      "00:21:12\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "#from gan_libs.DCGAN import build_generator, build_discriminator, build_functions\n",
    "# from gan_libs.LSGAN import build_generator, build_discriminator, build_functions\n",
    "# from gan_libs.SNGAN import build_generator, build_discriminator, build_functions\n",
    "# from gan_libs.WGAN_GP import build_generator, build_discriminator, build_functions\n",
    "\n",
    "from utils.common import set_gpu_config, predict_images\n",
    "from utils.draw_pose import draw_pose\n",
    "import numpy as np\n",
    "\n",
    "#set_gpu_config(\"0\",0.5)\n",
    "\n",
    "epoch = 25 + 1\n",
    "image_size = (1,1,105)\n",
    "noise_size = (1,1,5)\n",
    "batch_size = 16\n",
    "\n",
    "x_train = myData\n",
    "np.random.shuffle(x_train)\n",
    "\n",
    "generator = build_generator(noise_size)\n",
    "#print(generator.summary())\n",
    "discriminator = build_discriminator(image_size)\n",
    "#print(discriminator.summary())\n",
    "d_train, g_train = build_functions(batch_size, noise_size, image_size, generator, discriminator)\n",
    "\n",
    "\n",
    "number_of_all_data = x_train.shape[0]\n",
    "number_of_batches = int(number_of_all_data/batch_size)\n",
    "print('Number of Batches passed in each epoch: ',number_of_batches)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "is_first_epoch = True\n",
    "\n",
    "for e in range(epoch):\n",
    "    index = 0\n",
    "    for _ in range(number_of_batches):\n",
    "        real_images = x_train[index:index+batch_size]\n",
    "        index =+ batch_size\n",
    "        real_images.shape = (batch_size,1,1,105)\n",
    "        d_loss, = d_train([real_images, 1])\n",
    "        g_loss, = g_train([real_images, 1])\n",
    "     \n",
    "    \n",
    "    print (\"[{0}/{1}] d_loss: {2:.4}, g_loss: {3:.4}\".format(e, epoch, d_loss, g_loss))\n",
    "    #generating a sample\n",
    "    image = generator.predict(np.zeros(shape=(1,5)))\n",
    "    image = np.array(image)\n",
    "    draw_pose(image.reshape(105),'output',\"e{0}\".format(e))\n",
    "    \n",
    "    if(is_first_epoch):\n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('\\n\\nTime Taken for single epoch:')\n",
    "        print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "        is_first_epoch = False\n",
    "    \n",
    "    if e % 25 == 0 and e > 0:\n",
    "        generator.save_weights(\"e{0}_generator.h5\".format(e))\n",
    "        discriminator.save_weights(\"e{0}_discriminator.h5\".format(e))\n",
    " \n",
    "#just monitoring:\n",
    "##########################################################################################\n",
    "elapsed_time = time.time() - start_time \n",
    "print('\\n\\n\\n\\nNumber of parameter for the Generator and discriminator respectively:\\n')\n",
    "print(generator.count_params())\n",
    "print('')\n",
    "print(discriminator.count_params())\n",
    "print('\\n\\nNumber of Epochs and steps for each epoch:\\n')\n",
    "print('epochs: ',epoch, '   batches: ', number_of_batches)\n",
    "\n",
    "print('\\n\\nTime Taken:')\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "##########################################################################################\n",
    "#NOTE: previously in each epoch 1000 steps were iterated. in each iteration data was permitated and a batch was chosen. \n",
    "#now data is shuffled first and there are number_of_data/batch_size steps to be passed in a single epoch. \n",
    "# since 200 epochs and 1000 steps each was used, now 25 epochs and Ëœ7300 steps is used so the balance is kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[37mcbrc-All-Series\u001b[m  Mon May  6 14:40:03 2019\n",
      "\u001b[36m[0]\u001b[m \u001b[34mGeForce GTX 1080 Ti\u001b[m |\u001b[1m\u001b[31m 53'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 6283\u001b[m / \u001b[33m11172\u001b[m MB | \u001b[1m\u001b[30mcbrc\u001b[m(\u001b[33m5961M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m16M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m50M\u001b[m) \u001b[1m\u001b[30mcbrc\u001b[m(\u001b[33m114M\u001b[m) \u001b[1m\u001b[30mcbrc\u001b[m(\u001b[33m136M\u001b[m)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    #gpu details:\n",
    "    ################----------------------------\n",
    "from six.moves import cStringIO as StringIO\n",
    "import gpustat\n",
    "\n",
    "gpustats = gpustat.new_query()\n",
    "fp = StringIO()\n",
    "gpustats.print_formatted(\n",
    "     fp=fp, no_color=False, show_user=False,\n",
    "     show_cmd=False, show_pid=False, show_power=False, show_fan_speed=False)\n",
    "\n",
    "result = fp.getvalue()\n",
    "print('\\n\\n')\n",
    "print(result)\n",
    "    ################----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.load_weights(\"e400_generator.h5\".format(e))\n",
    "discriminator.load_weights(\"e400_discriminator.h5\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    x= np.array((i/100,0,0,0,0)).reshape(1,5)\n",
    "    image = generator.predict(x)\n",
    "    image = np.array(image)\n",
    "    draw_pose(image.reshape(105),'output',\"e{0}\".format(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
