{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Initial code was copied from:\n",
    "https://github.com/jason71995/Keras-GAN-Library\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,GlobalAveragePooling2D,LeakyReLU,Conv2DTranspose,Activation,BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras import initializers\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "\n",
    "\n",
    "noise_dim = 5\n",
    "\n",
    "def build_generator(input_shape):\n",
    "\n",
    "    generator = Sequential()\n",
    "    \n",
    "    generator.add(Dense(512,\n",
    "                        input_dim = noise_dim))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(512))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(512))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    \n",
    "    generator.add(Dense(512))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "\n",
    "    \n",
    "    generator.add(Dense(105, activation='tanh'))\n",
    "    return generator\n",
    "\n",
    "\n",
    "def build_discriminator(input_shape):\n",
    "\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Dense(512,\n",
    "                            input_dim=105))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "    \n",
    "    \n",
    "    discriminator.add(Dense(512))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "    \n",
    "    discriminator.add(Dense(512))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "        \n",
    "    discriminator.add(Dense(512))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "\n",
    "    \n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return discriminator\n",
    "\n",
    "def build_functions(batch_size, noise_size, image_size, generator, discriminator):\n",
    "\n",
    "    noise = K.random_normal((batch_size,) + noise_size,0.0,1.0,\"float32\")\n",
    "    real_image = K.placeholder((batch_size,) + image_size)\n",
    "\n",
    "    fake_image = generator(noise)\n",
    "\n",
    "    d_input = K.concatenate([real_image, fake_image], axis=0)\n",
    "    pred_real, pred_fake = tf.split(discriminator(d_input), num_or_size_splits = 2, axis = 0)\n",
    "\n",
    "    pred_real = K.clip(pred_real,K.epsilon(),1-K.epsilon())\n",
    "    pred_fake = K.clip(pred_fake,K.epsilon(),1-K.epsilon())\n",
    "\n",
    "    d_loss = -(K.mean(K.log(pred_real)) + K.mean(K.log(1-pred_fake)))\n",
    "    g_loss = -K.mean(K.log(pred_fake))\n",
    "\n",
    "    # get updates of mean and variance in batch normalization layers\n",
    "    d_updates = discriminator.get_updates_for([d_input])\n",
    "    g_updates = generator.get_updates_for([noise])\n",
    "\n",
    "    d_training_updates = Adam(lr=0.0001, beta_1=0.0, beta_2=0.9).get_updates(d_loss, discriminator.trainable_weights)\n",
    "    d_train = K.function([real_image, K.learning_phase()], [d_loss],d_updates + d_training_updates)\n",
    "\n",
    "    g_training_updates = Adam(lr=0.0001, beta_1=0.0, beta_2=0.9).get_updates(g_loss, generator.trainable_weights)\n",
    "    g_train = K.function([real_image, K.learning_phase()], [g_loss], g_updates + g_training_updates)\n",
    "\n",
    "    return d_train,g_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117562, 105)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading MHAD data for action1, all persons and all repeatations of each person\n",
    "from utils.data_loader import data_loader\n",
    "data_object= data_loader(matlab_action_path='../gan/')\n",
    "myData, mymin, mymax = data_object.actions_normalised([1], twoD_true_or_threeD_false=False)\n",
    "myData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 512)               3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 105)               53865     \n",
      "=================================================================\n",
      "Total params: 853,097\n",
      "Trainable params: 849,001\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 512)               54272     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 842,753\n",
      "Trainable params: 842,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[0/501] d_loss: 0.2997, g_loss: 4.162\n",
      "[1/501] d_loss: 0.4564, g_loss: 1.841\n",
      "[2/501] d_loss: 0.8462, g_loss: 2.172\n",
      "[3/501] d_loss: 0.7492, g_loss: 2.049\n",
      "[4/501] d_loss: 0.2402, g_loss: 4.103\n",
      "[5/501] d_loss: 1.017, g_loss: 6.188\n",
      "[6/501] d_loss: 0.2532, g_loss: 6.71\n",
      "[7/501] d_loss: 0.3844, g_loss: 7.864\n",
      "[8/501] d_loss: 0.4504, g_loss: 9.225\n",
      "[9/501] d_loss: 0.5324, g_loss: 5.36\n",
      "[10/501] d_loss: 0.6033, g_loss: 6.12\n",
      "[11/501] d_loss: 0.3124, g_loss: 9.621\n",
      "[12/501] d_loss: 0.1043, g_loss: 8.369\n",
      "[13/501] d_loss: 0.3602, g_loss: 8.454\n",
      "[14/501] d_loss: 0.4991, g_loss: 8.132\n",
      "[15/501] d_loss: 0.4531, g_loss: 7.26\n",
      "[16/501] d_loss: 0.9284, g_loss: 8.561\n",
      "[17/501] d_loss: 0.5389, g_loss: 9.924\n",
      "[18/501] d_loss: 1.272, g_loss: 9.738\n",
      "[19/501] d_loss: 0.5549, g_loss: 8.945\n",
      "[20/501] d_loss: 1.41, g_loss: 6.528\n",
      "[21/501] d_loss: 0.5845, g_loss: 6.689\n",
      "[22/501] d_loss: 1.193, g_loss: 4.578\n",
      "[23/501] d_loss: 0.7018, g_loss: 4.509\n",
      "[24/501] d_loss: 0.8192, g_loss: 6.537\n",
      "[25/501] d_loss: 0.6898, g_loss: 7.0\n",
      "[26/501] d_loss: 0.5543, g_loss: 4.018\n",
      "[27/501] d_loss: 1.052, g_loss: 5.093\n",
      "[28/501] d_loss: 1.08, g_loss: 3.572\n",
      "[29/501] d_loss: 0.9528, g_loss: 4.541\n",
      "[30/501] d_loss: 0.8489, g_loss: 6.028\n",
      "[31/501] d_loss: 0.9089, g_loss: 4.597\n",
      "[32/501] d_loss: 0.9128, g_loss: 5.475\n",
      "[33/501] d_loss: 1.093, g_loss: 4.086\n",
      "[34/501] d_loss: 1.021, g_loss: 2.878\n",
      "[35/501] d_loss: 1.223, g_loss: 5.614\n",
      "[36/501] d_loss: 0.9634, g_loss: 3.14\n",
      "[37/501] d_loss: 0.8631, g_loss: 7.644\n",
      "[38/501] d_loss: 1.011, g_loss: 2.869\n",
      "[39/501] d_loss: 1.048, g_loss: 4.246\n",
      "[40/501] d_loss: 1.085, g_loss: 4.853\n",
      "[41/501] d_loss: 1.101, g_loss: 3.851\n",
      "[42/501] d_loss: 0.9776, g_loss: 3.988\n",
      "[43/501] d_loss: 1.159, g_loss: 2.74\n",
      "[44/501] d_loss: 1.011, g_loss: 3.663\n",
      "[45/501] d_loss: 1.18, g_loss: 4.9\n",
      "[46/501] d_loss: 1.923, g_loss: 5.908\n",
      "[47/501] d_loss: 1.106, g_loss: 4.669\n",
      "[48/501] d_loss: 1.668, g_loss: 5.942\n",
      "[49/501] d_loss: 1.042, g_loss: 3.667\n",
      "[50/501] d_loss: 1.031, g_loss: 2.7\n",
      "[51/501] d_loss: 1.3, g_loss: 4.161\n",
      "[52/501] d_loss: 1.127, g_loss: 2.849\n",
      "[53/501] d_loss: 1.188, g_loss: 2.779\n",
      "[54/501] d_loss: 1.428, g_loss: 4.941\n",
      "[55/501] d_loss: 1.033, g_loss: 3.741\n",
      "[56/501] d_loss: 0.8292, g_loss: 3.904\n",
      "[57/501] d_loss: 1.259, g_loss: 3.327\n",
      "[58/501] d_loss: 1.138, g_loss: 2.759\n",
      "[59/501] d_loss: 1.332, g_loss: 2.83\n",
      "[60/501] d_loss: 1.51, g_loss: 2.673\n",
      "[61/501] d_loss: 1.289, g_loss: 3.547\n",
      "[62/501] d_loss: 1.451, g_loss: 2.917\n",
      "[63/501] d_loss: 1.396, g_loss: 4.621\n",
      "[64/501] d_loss: 1.047, g_loss: 3.622\n",
      "[65/501] d_loss: 0.8769, g_loss: 2.726\n",
      "[66/501] d_loss: 1.161, g_loss: 3.634\n",
      "[67/501] d_loss: 1.087, g_loss: 4.602\n",
      "[68/501] d_loss: 0.9655, g_loss: 2.703\n",
      "[69/501] d_loss: 1.133, g_loss: 2.504\n",
      "[70/501] d_loss: 1.19, g_loss: 2.978\n",
      "[71/501] d_loss: 0.9181, g_loss: 3.489\n",
      "[72/501] d_loss: 1.277, g_loss: 2.647\n",
      "[73/501] d_loss: 1.15, g_loss: 3.041\n",
      "[74/501] d_loss: 1.146, g_loss: 3.844\n",
      "[75/501] d_loss: 1.219, g_loss: 2.628\n",
      "[76/501] d_loss: 0.9628, g_loss: 3.311\n",
      "[85/501] d_loss: 1.114, g_loss: 5.925\n",
      "[86/501] d_loss: 1.116, g_loss: 2.689\n",
      "[87/501] d_loss: 0.9386, g_loss: 2.94\n",
      "[88/501] d_loss: 0.8118, g_loss: 3.053\n",
      "[89/501] d_loss: 1.083, g_loss: 2.728\n",
      "[90/501] d_loss: 1.092, g_loss: 4.315\n",
      "[91/501] d_loss: 1.053, g_loss: 3.686\n",
      "[92/501] d_loss: 1.208, g_loss: 2.635\n",
      "[93/501] d_loss: 0.829, g_loss: 4.998\n",
      "[94/501] d_loss: 1.203, g_loss: 3.645\n",
      "[95/501] d_loss: 1.083, g_loss: 2.942\n",
      "[96/501] d_loss: 1.246, g_loss: 2.942\n",
      "[97/501] d_loss: 1.03, g_loss: 4.216\n",
      "[98/501] d_loss: 1.249, g_loss: 2.743\n",
      "[99/501] d_loss: 1.141, g_loss: 2.701\n",
      "[100/501] d_loss: 1.027, g_loss: 3.452\n",
      "[101/501] d_loss: 1.272, g_loss: 2.684\n",
      "[102/501] d_loss: 1.182, g_loss: 3.376\n",
      "[103/501] d_loss: 1.308, g_loss: 2.817\n",
      "[104/501] d_loss: 1.308, g_loss: 2.76\n",
      "[105/501] d_loss: 1.225, g_loss: 2.766\n",
      "[106/501] d_loss: 1.172, g_loss: 2.892\n",
      "[107/501] d_loss: 1.031, g_loss: 2.853\n",
      "[108/501] d_loss: 1.089, g_loss: 2.718\n",
      "[109/501] d_loss: 1.439, g_loss: 2.765\n",
      "[110/501] d_loss: 1.159, g_loss: 3.792\n",
      "[111/501] d_loss: 1.121, g_loss: 2.669\n",
      "[112/501] d_loss: 1.102, g_loss: 2.823\n",
      "[113/501] d_loss: 1.145, g_loss: 3.098\n",
      "[114/501] d_loss: 1.047, g_loss: 2.798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115/501] d_loss: 1.204, g_loss: 3.817\n",
      "[116/501] d_loss: 1.176, g_loss: 2.621\n",
      "[117/501] d_loss: 1.163, g_loss: 2.751\n",
      "[118/501] d_loss: 1.133, g_loss: 2.701\n",
      "[119/501] d_loss: 1.422, g_loss: 2.564\n",
      "[120/501] d_loss: 1.197, g_loss: 2.739\n",
      "[121/501] d_loss: 1.124, g_loss: 2.663\n",
      "[122/501] d_loss: 1.07, g_loss: 2.742\n",
      "[123/501] d_loss: 1.147, g_loss: 2.893\n",
      "[124/501] d_loss: 1.248, g_loss: 2.727\n",
      "[125/501] d_loss: 1.109, g_loss: 2.853\n",
      "[126/501] d_loss: 1.325, g_loss: 3.346\n",
      "[127/501] d_loss: 1.238, g_loss: 2.816\n",
      "[128/501] d_loss: 1.433, g_loss: 3.774\n",
      "[129/501] d_loss: 0.9829, g_loss: 2.682\n",
      "[130/501] d_loss: 1.183, g_loss: 4.19\n",
      "[131/501] d_loss: 1.041, g_loss: 3.264\n",
      "[132/501] d_loss: 1.171, g_loss: 2.715\n",
      "[133/501] d_loss: 1.402, g_loss: 3.584\n",
      "[134/501] d_loss: 1.198, g_loss: 3.847\n",
      "[135/501] d_loss: 1.081, g_loss: 2.665\n",
      "[136/501] d_loss: 1.22, g_loss: 2.557\n",
      "[137/501] d_loss: 1.095, g_loss: 3.851\n",
      "[138/501] d_loss: 1.013, g_loss: 2.693\n",
      "[139/501] d_loss: 1.056, g_loss: 3.469\n",
      "[140/501] d_loss: 1.012, g_loss: 2.702\n",
      "[141/501] d_loss: 1.065, g_loss: 3.787\n",
      "[142/501] d_loss: 1.119, g_loss: 2.771\n",
      "[143/501] d_loss: 1.112, g_loss: 7.394\n",
      "[144/501] d_loss: 1.09, g_loss: 2.812\n",
      "[145/501] d_loss: 1.419, g_loss: 3.27\n",
      "[146/501] d_loss: 1.191, g_loss: 4.804\n",
      "[147/501] d_loss: 1.171, g_loss: 2.697\n",
      "[148/501] d_loss: 1.013, g_loss: 2.56\n",
      "[149/501] d_loss: 1.19, g_loss: 2.724\n",
      "[150/501] d_loss: 1.046, g_loss: 3.284\n",
      "[151/501] d_loss: 0.9357, g_loss: 3.675\n",
      "[152/501] d_loss: 1.194, g_loss: 2.595\n",
      "[153/501] d_loss: 1.108, g_loss: 2.564\n",
      "[154/501] d_loss: 1.196, g_loss: 3.652\n",
      "[155/501] d_loss: 1.171, g_loss: 3.425\n",
      "[156/501] d_loss: 1.048, g_loss: 2.837\n",
      "[157/501] d_loss: 1.155, g_loss: 2.616\n",
      "[158/501] d_loss: 1.048, g_loss: 2.877\n",
      "[159/501] d_loss: 0.9799, g_loss: 2.68\n",
      "[160/501] d_loss: 1.303, g_loss: 2.654\n",
      "[161/501] d_loss: 1.303, g_loss: 2.762\n",
      "[162/501] d_loss: 1.045, g_loss: 4.628\n",
      "[163/501] d_loss: 1.134, g_loss: 3.036\n",
      "[164/501] d_loss: 1.222, g_loss: 2.81\n",
      "[165/501] d_loss: 0.9791, g_loss: 2.59\n",
      "[166/501] d_loss: 1.34, g_loss: 2.816\n",
      "[167/501] d_loss: 1.055, g_loss: 2.844\n",
      "[168/501] d_loss: 1.047, g_loss: 2.946\n",
      "[169/501] d_loss: 1.085, g_loss: 2.195\n",
      "[170/501] d_loss: 1.161, g_loss: 2.699\n",
      "[171/501] d_loss: 1.209, g_loss: 2.795\n",
      "[172/501] d_loss: 1.053, g_loss: 2.743\n",
      "[173/501] d_loss: 1.012, g_loss: 3.118\n",
      "[174/501] d_loss: 0.9765, g_loss: 3.584\n",
      "[175/501] d_loss: 1.201, g_loss: 2.824\n",
      "[176/501] d_loss: 1.226, g_loss: 3.155\n",
      "[177/501] d_loss: 1.243, g_loss: 2.98\n",
      "[178/501] d_loss: 1.119, g_loss: 2.702\n",
      "[179/501] d_loss: 1.394, g_loss: 3.656\n",
      "[180/501] d_loss: 1.192, g_loss: 2.616\n",
      "[181/501] d_loss: 1.141, g_loss: 2.804\n",
      "[182/501] d_loss: 1.164, g_loss: 2.735\n",
      "[183/501] d_loss: 1.063, g_loss: 4.719\n",
      "[184/501] d_loss: 1.167, g_loss: 2.747\n",
      "[185/501] d_loss: 1.041, g_loss: 2.694\n",
      "[186/501] d_loss: 0.9206, g_loss: 3.02\n",
      "[187/501] d_loss: 0.9628, g_loss: 2.695\n",
      "[188/501] d_loss: 1.106, g_loss: 3.047\n",
      "[189/501] d_loss: 1.125, g_loss: 2.626\n",
      "[190/501] d_loss: 1.035, g_loss: 4.016\n",
      "[191/501] d_loss: 1.061, g_loss: 2.522\n",
      "[192/501] d_loss: 1.162, g_loss: 2.912\n",
      "[193/501] d_loss: 0.7567, g_loss: 2.563\n",
      "[194/501] d_loss: 1.196, g_loss: 2.818\n",
      "[195/501] d_loss: 0.9296, g_loss: 3.149\n",
      "[196/501] d_loss: 1.089, g_loss: 2.794\n",
      "[197/501] d_loss: 1.223, g_loss: 2.649\n",
      "[198/501] d_loss: 1.178, g_loss: 2.761\n",
      "[199/501] d_loss: 0.9807, g_loss: 2.51\n",
      "[200/501] d_loss: 1.077, g_loss: 2.73\n",
      "[201/501] d_loss: 1.152, g_loss: 2.615\n",
      "[202/501] d_loss: 1.206, g_loss: 2.77\n",
      "[203/501] d_loss: 1.041, g_loss: 2.75\n",
      "[204/501] d_loss: 0.9089, g_loss: 2.675\n",
      "[205/501] d_loss: 1.212, g_loss: 2.878\n",
      "[206/501] d_loss: 1.337, g_loss: 2.724\n",
      "[207/501] d_loss: 1.208, g_loss: 2.678\n",
      "[208/501] d_loss: 0.9261, g_loss: 2.541\n",
      "[209/501] d_loss: 0.9279, g_loss: 4.529\n",
      "[210/501] d_loss: 1.173, g_loss: 1.997\n",
      "[211/501] d_loss: 1.153, g_loss: 1.913\n",
      "[212/501] d_loss: 1.008, g_loss: 1.934\n",
      "[213/501] d_loss: 1.115, g_loss: 2.116\n",
      "[214/501] d_loss: 1.47, g_loss: 2.533\n",
      "[215/501] d_loss: 1.23, g_loss: 2.173\n",
      "[216/501] d_loss: 1.27, g_loss: 2.035\n",
      "[217/501] d_loss: 0.9878, g_loss: 1.7\n",
      "[218/501] d_loss: 1.707, g_loss: 3.597\n",
      "[219/501] d_loss: 1.217, g_loss: 1.745\n",
      "[220/501] d_loss: 1.202, g_loss: 1.667\n",
      "[221/501] d_loss: 1.339, g_loss: 2.423\n",
      "[222/501] d_loss: 1.316, g_loss: 2.042\n",
      "[223/501] d_loss: 1.328, g_loss: 2.138\n",
      "[224/501] d_loss: 1.401, g_loss: 1.788\n",
      "[225/501] d_loss: 1.309, g_loss: 1.846\n",
      "[226/501] d_loss: 1.345, g_loss: 1.645\n",
      "[227/501] d_loss: 1.245, g_loss: 1.767\n",
      "[228/501] d_loss: 1.196, g_loss: 5.164\n",
      "[229/501] d_loss: 0.9953, g_loss: 1.76\n",
      "[230/501] d_loss: 1.119, g_loss: 1.775\n",
      "[231/501] d_loss: 1.24, g_loss: 1.654\n",
      "[232/501] d_loss: 1.346, g_loss: 1.96\n",
      "[233/501] d_loss: 1.156, g_loss: 1.839\n",
      "[234/501] d_loss: 1.238, g_loss: 1.74\n",
      "[235/501] d_loss: 0.881, g_loss: 1.809\n",
      "[236/501] d_loss: 1.201, g_loss: 1.922\n",
      "[237/501] d_loss: 1.245, g_loss: 1.788\n",
      "[238/501] d_loss: 1.025, g_loss: 2.083\n",
      "[239/501] d_loss: 1.087, g_loss: 2.377\n",
      "[240/501] d_loss: 1.059, g_loss: 1.511\n",
      "[241/501] d_loss: 0.9832, g_loss: 3.652\n",
      "[242/501] d_loss: 0.8887, g_loss: 1.745\n",
      "[243/501] d_loss: 1.245, g_loss: 2.678\n",
      "[244/501] d_loss: 1.339, g_loss: 1.683\n",
      "[245/501] d_loss: 1.244, g_loss: 1.835\n",
      "[246/501] d_loss: 1.057, g_loss: 1.789\n",
      "[247/501] d_loss: 1.196, g_loss: 1.7\n",
      "[248/501] d_loss: 1.051, g_loss: 2.017\n",
      "[249/501] d_loss: 1.466, g_loss: 1.873\n",
      "[250/501] d_loss: 1.345, g_loss: 2.002\n",
      "[251/501] d_loss: 1.268, g_loss: 2.331\n",
      "[252/501] d_loss: 1.339, g_loss: 1.712\n",
      "[253/501] d_loss: 1.166, g_loss: 2.073\n",
      "[254/501] d_loss: 1.402, g_loss: 2.277\n",
      "[255/501] d_loss: 1.145, g_loss: 2.715\n",
      "[256/501] d_loss: 1.078, g_loss: 1.89\n",
      "[257/501] d_loss: 1.206, g_loss: 1.794\n",
      "[258/501] d_loss: 1.38, g_loss: 2.779\n",
      "[259/501] d_loss: 1.327, g_loss: 1.652\n",
      "[260/501] d_loss: 1.328, g_loss: 1.96\n",
      "[261/501] d_loss: 1.297, g_loss: 1.742\n",
      "[262/501] d_loss: 1.439, g_loss: 2.956\n",
      "[263/501] d_loss: 1.284, g_loss: 1.678\n",
      "[264/501] d_loss: 1.359, g_loss: 1.697\n",
      "[265/501] d_loss: 1.284, g_loss: 2.065\n",
      "[266/501] d_loss: 1.041, g_loss: 1.785\n",
      "[267/501] d_loss: 1.182, g_loss: 2.181\n",
      "[268/501] d_loss: 1.194, g_loss: 2.111\n",
      "[269/501] d_loss: 1.324, g_loss: 1.765\n",
      "[270/501] d_loss: 1.242, g_loss: 3.317\n",
      "[271/501] d_loss: 1.192, g_loss: 1.823\n",
      "[272/501] d_loss: 1.391, g_loss: 1.76\n",
      "[273/501] d_loss: 1.213, g_loss: 2.577\n",
      "[274/501] d_loss: 1.316, g_loss: 1.661\n",
      "[275/501] d_loss: 1.128, g_loss: 1.717\n",
      "[276/501] d_loss: 1.32, g_loss: 1.767\n",
      "[277/501] d_loss: 1.233, g_loss: 2.213\n",
      "[278/501] d_loss: 1.218, g_loss: 1.727\n",
      "[279/501] d_loss: 1.165, g_loss: 1.702\n",
      "[280/501] d_loss: 1.19, g_loss: 1.746\n",
      "[281/501] d_loss: 1.129, g_loss: 1.716\n",
      "[282/501] d_loss: 1.134, g_loss: 1.732\n",
      "[283/501] d_loss: 1.38, g_loss: 1.917\n",
      "[284/501] d_loss: 1.534, g_loss: 2.108\n",
      "[285/501] d_loss: 1.342, g_loss: 2.559\n",
      "[286/501] d_loss: 1.34, g_loss: 1.755\n",
      "[287/501] d_loss: 1.319, g_loss: 1.855\n",
      "[288/501] d_loss: 1.134, g_loss: 2.908\n",
      "[289/501] d_loss: 1.234, g_loss: 2.095\n",
      "[290/501] d_loss: 1.304, g_loss: 1.893\n",
      "[291/501] d_loss: 1.308, g_loss: 1.855\n",
      "[292/501] d_loss: 1.301, g_loss: 1.816\n",
      "[293/501] d_loss: 1.155, g_loss: 1.715\n",
      "[294/501] d_loss: 1.199, g_loss: 1.704\n",
      "[295/501] d_loss: 1.337, g_loss: 1.661\n",
      "[296/501] d_loss: 1.366, g_loss: 1.615\n",
      "[297/501] d_loss: 1.31, g_loss: 1.798\n",
      "[298/501] d_loss: 1.128, g_loss: 1.758\n",
      "[299/501] d_loss: 1.316, g_loss: 1.741\n",
      "[300/501] d_loss: 1.202, g_loss: 1.881\n",
      "[301/501] d_loss: 1.348, g_loss: 2.033\n",
      "[302/501] d_loss: 1.335, g_loss: 1.637\n",
      "[303/501] d_loss: 1.013, g_loss: 1.66\n",
      "[304/501] d_loss: 1.173, g_loss: 1.935\n",
      "[305/501] d_loss: 1.244, g_loss: 5.565\n",
      "[306/501] d_loss: 1.156, g_loss: 1.974\n",
      "[307/501] d_loss: 1.4, g_loss: 1.804\n",
      "[308/501] d_loss: 1.442, g_loss: 1.642\n",
      "[309/501] d_loss: 1.286, g_loss: 1.732\n",
      "[310/501] d_loss: 1.133, g_loss: 2.583\n",
      "[311/501] d_loss: 1.31, g_loss: 1.697\n",
      "[312/501] d_loss: 1.25, g_loss: 1.691\n",
      "[313/501] d_loss: 1.316, g_loss: 2.271\n",
      "[314/501] d_loss: 1.269, g_loss: 1.638\n",
      "[315/501] d_loss: 1.188, g_loss: 1.672\n",
      "[316/501] d_loss: 1.328, g_loss: 1.975\n",
      "[317/501] d_loss: 1.449, g_loss: 1.72\n",
      "[318/501] d_loss: 1.059, g_loss: 1.734\n",
      "[319/501] d_loss: 1.149, g_loss: 1.929\n",
      "[320/501] d_loss: 1.308, g_loss: 1.929\n",
      "[321/501] d_loss: 1.172, g_loss: 2.793\n",
      "[322/501] d_loss: 1.166, g_loss: 2.703\n",
      "[323/501] d_loss: 1.254, g_loss: 1.754\n",
      "[324/501] d_loss: 0.9484, g_loss: 2.777\n",
      "[325/501] d_loss: 1.287, g_loss: 1.728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[326/501] d_loss: 1.216, g_loss: 1.631\n",
      "[327/501] d_loss: 1.136, g_loss: 1.968\n",
      "[328/501] d_loss: 1.041, g_loss: 1.691\n",
      "[329/501] d_loss: 1.188, g_loss: 1.745\n",
      "[330/501] d_loss: 1.179, g_loss: 1.732\n",
      "[331/501] d_loss: 1.256, g_loss: 1.857\n",
      "[332/501] d_loss: 1.193, g_loss: 1.773\n",
      "[333/501] d_loss: 1.436, g_loss: 2.098\n",
      "[334/501] d_loss: 1.246, g_loss: 1.885\n",
      "[335/501] d_loss: 1.135, g_loss: 2.438\n",
      "[336/501] d_loss: 1.26, g_loss: 2.012\n",
      "[337/501] d_loss: 1.071, g_loss: 1.684\n",
      "[338/501] d_loss: 1.13, g_loss: 1.624\n",
      "[339/501] d_loss: 0.9783, g_loss: 1.653\n",
      "[340/501] d_loss: 1.239, g_loss: 1.914\n",
      "[341/501] d_loss: 1.124, g_loss: 1.835\n",
      "[342/501] d_loss: 1.245, g_loss: 1.692\n",
      "[343/501] d_loss: 1.128, g_loss: 1.795\n",
      "[344/501] d_loss: 0.9347, g_loss: 3.419\n",
      "[345/501] d_loss: 1.221, g_loss: 2.13\n",
      "[346/501] d_loss: 1.208, g_loss: 1.715\n",
      "[347/501] d_loss: 1.317, g_loss: 1.854\n",
      "[348/501] d_loss: 1.25, g_loss: 1.917\n",
      "[349/501] d_loss: 1.318, g_loss: 1.765\n",
      "[350/501] d_loss: 1.169, g_loss: 3.545\n",
      "[351/501] d_loss: 1.369, g_loss: 1.712\n",
      "[352/501] d_loss: 1.153, g_loss: 1.848\n",
      "[353/501] d_loss: 1.222, g_loss: 1.944\n",
      "[354/501] d_loss: 1.219, g_loss: 1.892\n",
      "[355/501] d_loss: 1.34, g_loss: 1.737\n",
      "[356/501] d_loss: 1.249, g_loss: 1.884\n",
      "[357/501] d_loss: 1.191, g_loss: 2.724\n",
      "[358/501] d_loss: 1.354, g_loss: 1.603\n",
      "[359/501] d_loss: 1.293, g_loss: 1.911\n",
      "[360/501] d_loss: 1.323, g_loss: 1.868\n",
      "[361/501] d_loss: 1.177, g_loss: 1.585\n",
      "[362/501] d_loss: 1.122, g_loss: 9.425\n",
      "[363/501] d_loss: 1.196, g_loss: 2.138\n",
      "[364/501] d_loss: 1.056, g_loss: 1.937\n",
      "[365/501] d_loss: 1.298, g_loss: 1.86\n",
      "[366/501] d_loss: 1.281, g_loss: 2.168\n",
      "[367/501] d_loss: 1.295, g_loss: 2.932\n",
      "[368/501] d_loss: 1.315, g_loss: 1.74\n",
      "[369/501] d_loss: 1.312, g_loss: 2.165\n",
      "[370/501] d_loss: 1.114, g_loss: 1.957\n",
      "[371/501] d_loss: 1.259, g_loss: 1.571\n",
      "[372/501] d_loss: 1.267, g_loss: 4.322\n",
      "[373/501] d_loss: 1.214, g_loss: 1.887\n",
      "[374/501] d_loss: 1.077, g_loss: 1.65\n",
      "[375/501] d_loss: 1.27, g_loss: 6.502\n",
      "[376/501] d_loss: 1.323, g_loss: 1.82\n",
      "[377/501] d_loss: 1.265, g_loss: 1.643\n",
      "[378/501] d_loss: 1.257, g_loss: 1.948\n",
      "[379/501] d_loss: 1.196, g_loss: 1.976\n",
      "[380/501] d_loss: 1.158, g_loss: 1.8\n",
      "[381/501] d_loss: 1.115, g_loss: 2.109\n",
      "[382/501] d_loss: 1.097, g_loss: 1.637\n",
      "[383/501] d_loss: 1.049, g_loss: 1.903\n",
      "[384/501] d_loss: 1.064, g_loss: 1.804\n",
      "[385/501] d_loss: 1.089, g_loss: 1.798\n",
      "[386/501] d_loss: 1.133, g_loss: 1.699\n",
      "[387/501] d_loss: 0.8229, g_loss: 1.978\n",
      "[388/501] d_loss: 1.247, g_loss: 1.649\n",
      "[389/501] d_loss: 1.561, g_loss: 2.328\n",
      "[390/501] d_loss: 2.269, g_loss: 3.409\n",
      "[391/501] d_loss: 1.378, g_loss: 2.974\n",
      "[392/501] d_loss: 1.349, g_loss: 1.964\n",
      "[393/501] d_loss: 1.175, g_loss: 2.048\n",
      "[394/501] d_loss: 1.148, g_loss: 1.84\n",
      "[395/501] d_loss: 1.327, g_loss: 1.818\n",
      "[396/501] d_loss: 1.085, g_loss: 2.861\n",
      "[397/501] d_loss: 1.127, g_loss: 1.811\n",
      "[398/501] d_loss: 1.218, g_loss: 1.881\n",
      "[399/501] d_loss: 1.241, g_loss: 1.79\n",
      "[400/501] d_loss: 1.311, g_loss: 1.888\n",
      "[401/501] d_loss: 1.158, g_loss: 2.083\n",
      "[402/501] d_loss: 1.272, g_loss: 1.984\n",
      "[403/501] d_loss: 1.282, g_loss: 1.827\n",
      "[404/501] d_loss: 1.54, g_loss: 1.694\n",
      "[405/501] d_loss: 1.107, g_loss: 1.901\n",
      "[406/501] d_loss: 1.439, g_loss: 2.198\n",
      "[407/501] d_loss: 1.166, g_loss: 1.831\n",
      "[408/501] d_loss: 1.066, g_loss: 3.373\n",
      "[409/501] d_loss: 1.308, g_loss: 2.004\n",
      "[410/501] d_loss: 1.206, g_loss: 1.797\n",
      "[411/501] d_loss: 1.232, g_loss: 1.655\n",
      "[412/501] d_loss: 1.357, g_loss: 1.831\n",
      "[413/501] d_loss: 1.362, g_loss: 1.719\n",
      "[414/501] d_loss: 1.179, g_loss: 1.903\n",
      "[415/501] d_loss: 1.212, g_loss: 1.931\n",
      "[416/501] d_loss: 1.12, g_loss: 1.821\n",
      "[417/501] d_loss: 1.372, g_loss: 1.819\n",
      "[418/501] d_loss: 1.228, g_loss: 1.62\n",
      "[419/501] d_loss: 1.214, g_loss: 1.755\n",
      "[420/501] d_loss: 1.254, g_loss: 1.702\n",
      "[421/501] d_loss: 1.067, g_loss: 1.699\n",
      "[422/501] d_loss: 1.273, g_loss: 10.79\n",
      "[423/501] d_loss: 1.205, g_loss: 1.71\n",
      "[424/501] d_loss: 1.27, g_loss: 1.86\n",
      "[425/501] d_loss: 1.058, g_loss: 1.781\n",
      "[426/501] d_loss: 1.223, g_loss: 1.93\n",
      "[427/501] d_loss: 1.238, g_loss: 1.569\n",
      "[428/501] d_loss: 1.168, g_loss: 1.687\n",
      "[429/501] d_loss: 1.043, g_loss: 3.396\n",
      "[430/501] d_loss: 1.203, g_loss: 1.918\n",
      "[431/501] d_loss: 1.175, g_loss: 2.988\n",
      "[432/501] d_loss: 0.8528, g_loss: 2.096\n",
      "[433/501] d_loss: 1.202, g_loss: 2.959\n",
      "[434/501] d_loss: 1.376, g_loss: 2.388\n",
      "[435/501] d_loss: 1.296, g_loss: 3.199\n",
      "[436/501] d_loss: 0.9254, g_loss: 1.764\n",
      "[437/501] d_loss: 1.21, g_loss: 1.999\n",
      "[438/501] d_loss: 1.156, g_loss: 2.012\n",
      "[439/501] d_loss: 1.05, g_loss: 1.818\n",
      "[440/501] d_loss: 1.385, g_loss: 1.812\n",
      "[441/501] d_loss: 1.191, g_loss: 6.843\n",
      "[442/501] d_loss: 1.264, g_loss: 2.129\n",
      "[443/501] d_loss: 1.088, g_loss: 1.734\n",
      "[444/501] d_loss: 1.299, g_loss: 2.202\n",
      "[445/501] d_loss: 1.116, g_loss: 1.651\n",
      "[446/501] d_loss: 1.448, g_loss: 1.904\n",
      "[447/501] d_loss: 1.181, g_loss: 1.722\n",
      "[448/501] d_loss: 1.212, g_loss: 1.903\n",
      "[449/501] d_loss: 1.364, g_loss: 1.722\n",
      "[450/501] d_loss: 1.263, g_loss: 1.816\n",
      "[451/501] d_loss: 1.298, g_loss: 1.948\n",
      "[452/501] d_loss: 1.181, g_loss: 1.877\n",
      "[453/501] d_loss: 1.129, g_loss: 1.684\n",
      "[454/501] d_loss: 1.403, g_loss: 1.678\n",
      "[455/501] d_loss: 1.022, g_loss: 2.375\n",
      "[456/501] d_loss: 1.328, g_loss: 1.746\n",
      "[457/501] d_loss: 1.326, g_loss: 1.949\n",
      "[458/501] d_loss: 1.1, g_loss: 1.747\n",
      "[459/501] d_loss: 1.139, g_loss: 2.05\n",
      "[460/501] d_loss: 1.33, g_loss: 1.969\n",
      "[461/501] d_loss: 1.211, g_loss: 1.699\n",
      "[462/501] d_loss: 1.14, g_loss: 1.976\n",
      "[463/501] d_loss: 1.377, g_loss: 1.864\n",
      "[464/501] d_loss: 1.062, g_loss: 1.934\n",
      "[465/501] d_loss: 1.173, g_loss: 1.756\n",
      "[466/501] d_loss: 1.247, g_loss: 1.968\n",
      "[467/501] d_loss: 1.227, g_loss: 1.829\n",
      "[468/501] d_loss: 1.477, g_loss: 1.812\n",
      "[469/501] d_loss: 1.222, g_loss: 2.152\n",
      "[470/501] d_loss: 1.463, g_loss: 2.609\n",
      "[471/501] d_loss: 1.233, g_loss: 1.754\n",
      "[472/501] d_loss: 1.238, g_loss: 1.682\n",
      "[473/501] d_loss: 1.311, g_loss: 1.819\n",
      "[474/501] d_loss: 1.38, g_loss: 1.763\n",
      "[475/501] d_loss: 1.146, g_loss: 1.869\n",
      "[476/501] d_loss: 1.109, g_loss: 2.595\n",
      "[477/501] d_loss: 1.376, g_loss: 1.643\n",
      "[478/501] d_loss: 1.14, g_loss: 1.939\n",
      "[479/501] d_loss: 1.321, g_loss: 1.891\n",
      "[480/501] d_loss: 1.042, g_loss: 1.779\n",
      "[481/501] d_loss: 1.256, g_loss: 1.742\n",
      "[482/501] d_loss: 1.061, g_loss: 1.855\n",
      "[483/501] d_loss: 1.158, g_loss: 1.964\n",
      "[484/501] d_loss: 1.153, g_loss: 3.7\n",
      "[485/501] d_loss: 1.125, g_loss: 1.64\n",
      "[486/501] d_loss: 1.354, g_loss: 2.034\n",
      "[487/501] d_loss: 1.005, g_loss: 1.835\n",
      "[488/501] d_loss: 1.35, g_loss: 1.935\n",
      "[489/501] d_loss: 0.964, g_loss: 1.866\n",
      "[490/501] d_loss: 0.9909, g_loss: 3.074\n",
      "[491/501] d_loss: 1.125, g_loss: 1.656\n",
      "[492/501] d_loss: 1.433, g_loss: 1.928\n",
      "[493/501] d_loss: 1.34, g_loss: 2.54\n",
      "[494/501] d_loss: 1.215, g_loss: 1.769\n",
      "[495/501] d_loss: 0.9397, g_loss: 2.173\n",
      "[496/501] d_loss: 1.177, g_loss: 1.785\n",
      "[497/501] d_loss: 1.209, g_loss: 1.941\n",
      "[498/501] d_loss: 1.369, g_loss: 1.807\n",
      "[499/501] d_loss: 1.199, g_loss: 1.923\n",
      "[500/501] d_loss: 1.083, g_loss: 1.728\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "#from gan_libs.DCGAN import build_generator, build_discriminator, build_functions\n",
    "# from gan_libs.LSGAN import build_generator, build_discriminator, build_functions\n",
    "# from gan_libs.SNGAN import build_generator, build_discriminator, build_functions\n",
    "# from gan_libs.WGAN_GP import build_generator, build_discriminator, build_functions\n",
    "\n",
    "from utils.common import set_gpu_config, predict_images\n",
    "from utils.draw_pose import draw_pose\n",
    "import numpy as np\n",
    "\n",
    "set_gpu_config(\"0\",0.5)\n",
    "\n",
    "epoch = 500 + 1\n",
    "steps = 1000\n",
    "image_size = (1,1,105)\n",
    "noise_size = (1,1,5)\n",
    "batch_size = 16\n",
    "\n",
    "x_train = myData\n",
    "\n",
    "generator = build_generator(noise_size)\n",
    "print(generator.summary())\n",
    "discriminator = build_discriminator(image_size)\n",
    "print(discriminator.summary())\n",
    "d_train, g_train = build_functions(batch_size, noise_size, image_size, generator, discriminator)\n",
    "\n",
    "for e in range(epoch):\n",
    "    for s in range(steps):\n",
    "        real_images = x_train[np.random.permutation(x_train.shape[0])[:batch_size]]\n",
    "        real_images.shape = (batch_size,1,1,105)\n",
    "        d_loss, = d_train([real_images, 1])\n",
    "        g_loss, = g_train([real_images, 1])\n",
    "     \n",
    "    \n",
    "    print (\"[{0}/{1}] d_loss: {2:.4}, g_loss: {3:.4}\".format(e, epoch, d_loss, g_loss))\n",
    "    #generating a sample\n",
    "    image = generator.predict(np.zeros(shape=(1,5)))\n",
    "    image = np.array(image)\n",
    "    draw_pose(image.reshape(105),'output',\"e{0}\".format(e))\n",
    "        \n",
    "    if e % 100 == 0:\n",
    "        generator.save_weights(\"e{0}_generator.h5\".format(e))\n",
    "        discriminator.save_weights(\"e{0}_discriminator.h5\".format(e))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.load_weights(\"e400_generator.h5\".format(e))\n",
    "discriminator.load_weights(\"e400_discriminator.h5\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    x= np.array((i/100,0,0,0,0)).reshape(1,5)\n",
    "    image = generator.predict(x)\n",
    "    image = np.array(image)\n",
    "    draw_pose(image.reshape(105),'output',\"e{0}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
