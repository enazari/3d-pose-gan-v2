{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Initial code was copied from:\n",
    "https://github.com/eurismarpires/Keras-GAN-1/blob/master/gan/gan.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\n",
    "    Ref:\n",
    "        - https://arxiv.org/abs/1511.06434\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,GlobalAveragePooling2D,LeakyReLU,Conv2DTranspose,Activation,BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras import initializers\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_generator(input_shape):\n",
    "#     model = Sequential()\n",
    "\n",
    "#     model.add(Conv2DTranspose(512,(3,3),strides=(2,2),padding=\"same\",input_shape=input_shape))\n",
    "#     model.add(LeakyReLU(0.2))\n",
    "#     model.add(BatchNormalization())\n",
    "\n",
    "#     model.add(Conv2DTranspose(256,(3,3),strides=(2,2),padding=\"same\"))\n",
    "#     model.add(LeakyReLU(0.2))\n",
    "#     model.add(BatchNormalization())\n",
    "\n",
    "#     model.add(Conv2DTranspose(128,(3,3),strides=(2,2),padding=\"same\"))\n",
    "#     model.add(LeakyReLU(0.2))\n",
    "#     model.add(BatchNormalization())\n",
    "\n",
    "#     model.add(Conv2DTranspose(64,(3,3),strides=(2,2),padding=\"same\"))\n",
    "#     model.add(LeakyReLU(0.2))\n",
    "#     model.add(BatchNormalization())\n",
    "\n",
    "#     model.add(Conv2D(3,(3,3),padding=\"same\",activation=\"tanh\"))\n",
    "\n",
    "    generator = Sequential()\n",
    "    \n",
    "    generator.add(Dense(512,\n",
    "                        input_dim=5,#noise_dim,\n",
    "                        kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(512))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(512))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "\n",
    "    \n",
    "    generator.add(Dense(512))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    \n",
    "    generator.add(Dense(512))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    \n",
    "    generator.add(Dense(105, activation='tanh'))\n",
    "    return generator\n",
    "\n",
    "\n",
    "def build_discriminator(input_shape):\n",
    "#     model = Sequential()\n",
    "\n",
    "#     model.add(Conv2D(64,(3,3),strides=(2,2),padding=\"same\",input_shape=input_shape))\n",
    "#     model.add(LeakyReLU(0.2))\n",
    "#     model.add(BatchNormalization())\n",
    "\n",
    "#     model.add(Conv2D(128,(3,3),strides=(2,2),padding=\"same\"))\n",
    "#     model.add(LeakyReLU(0.2))\n",
    "#     model.add(BatchNormalization())\n",
    "\n",
    "#     model.add(Conv2D(256,(3,3),strides=(2,2),padding=\"same\"))\n",
    "#     model.add(LeakyReLU(0.2))\n",
    "#     model.add(BatchNormalization())\n",
    "\n",
    "#     model.add(Conv2D(512,(3,3),strides=(2,2),padding=\"same\"))\n",
    "#     model.add(LeakyReLU(0.2))\n",
    "#     model.add(BatchNormalization())\n",
    "\n",
    "#     model.add(Conv2D(1,(3,3),padding=\"same\"))\n",
    "#     model.add(GlobalAveragePooling2D())\n",
    "#     model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Dense(512,\n",
    "                            input_dim=105,\n",
    "                            kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "    \n",
    "    \n",
    "    discriminator.add(Dense(512))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "    \n",
    "    discriminator.add(Dense(512))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "        \n",
    "    discriminator.add(Dense(512))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "        \n",
    "    discriminator.add(Dense(512))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "    \n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return discriminator\n",
    "\n",
    "def build_functions(batch_size, noise_size, image_size, generator, discriminator):\n",
    "\n",
    "    noise = K.random_normal((batch_size,) + noise_size,0.0,1.0,\"float32\")\n",
    "    real_image = K.placeholder((batch_size,) + image_size)\n",
    "\n",
    "    fake_image = generator(noise)\n",
    "\n",
    "    d_input = K.concatenate([real_image, fake_image], axis=0)\n",
    "    pred_real, pred_fake = tf.split(discriminator(d_input), num_or_size_splits = 2, axis = 0)\n",
    "\n",
    "    pred_real = K.clip(pred_real,K.epsilon(),1-K.epsilon())\n",
    "    pred_fake = K.clip(pred_fake,K.epsilon(),1-K.epsilon())\n",
    "\n",
    "    d_loss = -(K.mean(K.log(pred_real)) + K.mean(K.log(1-pred_fake)))\n",
    "    g_loss = -K.mean(K.log(pred_fake))\n",
    "\n",
    "    # get updates of mean and variance in batch normalization layers\n",
    "    d_updates = discriminator.get_updates_for([d_input])\n",
    "    g_updates = generator.get_updates_for([noise])\n",
    "\n",
    "    d_training_updates = Adam(lr=0.0001, beta_1=0.0, beta_2=0.9).get_updates(d_loss, discriminator.trainable_weights)\n",
    "    d_train = K.function([real_image, K.learning_phase()], [d_loss],d_updates + d_training_updates)\n",
    "\n",
    "    g_training_updates = Adam(lr=0.0001, beta_1=0.0, beta_2=0.9).get_updates(g_loss, generator.trainable_weights)\n",
    "    g_train = K.function([real_image, K.learning_phase()], [g_loss], g_updates + g_training_updates)\n",
    "\n",
    "    return d_train,g_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MHAD data for action1, all persons and all repeatations of each person\n",
    "from utils.data_loader import data_loader\n",
    "data_object= data_loader(matlab_action_path='../../')\n",
    "myData, mymin, mymax = data_object.actions_normalised([1], twoD_true_or_threeD_false=False)\n",
    "myData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "#from gan_libs.DCGAN import build_generator, build_discriminator, build_functions\n",
    "# from gan_libs.LSGAN import build_generator, build_discriminator, build_functions\n",
    "# from gan_libs.SNGAN import build_generator, build_discriminator, build_functions\n",
    "# from gan_libs.WGAN_GP import build_generator, build_discriminator, build_functions\n",
    "\n",
    "from utils.common import set_gpu_config, predict_images\n",
    "from utils.draw_pose import draw_pose\n",
    "import numpy as np\n",
    "\n",
    "set_gpu_config(\"0\",0.5)\n",
    "\n",
    "epoch = 500\n",
    "steps = 1000\n",
    "image_size = (1,1,105)\n",
    "noise_size = (1,1,5)\n",
    "batch_size = 16\n",
    "\n",
    "x_train = myData\n",
    "\n",
    "generator = build_generator(noise_size)\n",
    "discriminator = build_discriminator(image_size)\n",
    "d_train, g_train = build_functions(batch_size, noise_size, image_size, generator, discriminator)\n",
    "\n",
    "for e in range(epoch):\n",
    "    for s in range(steps):\n",
    "        real_images = x_train[np.random.permutation(x_train.shape[0])[:batch_size]]\n",
    "        real_images.shape = (batch_size,1,1,105)\n",
    "        d_loss, = d_train([real_images, 1])\n",
    "        g_loss, = g_train([real_images, 1])\n",
    "        print (\"[{0}/{1}] [{2}/{3}] d_loss: {4:.4}, g_loss: {5:.4}\".format(e, epoch, s, steps, d_loss, g_loss))\n",
    "        \n",
    "        image = generator.predict(np.random.normal(size=(1,5)))\n",
    "        image = np.array(image)\n",
    "        draw_pose(image.reshape(105),'output',\"e{0}_s{1}\".format(e,s))\n",
    "    if e % 100 == 0:\n",
    "        generator.save_weights(\"e{0}_generator.h5\".format(e))\n",
    "        discriminator.save_weights(\"e{0}_discriminator.h5\".format(e))\n",
    "    #predict_images(\"e{0}_img.png\".format(e), generator,noise_size,10,32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
